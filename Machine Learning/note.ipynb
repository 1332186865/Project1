{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 引言"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K临近算法\n",
    "1.  将数据拆分为训练数据与测试数据\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data']数据, iris_dataset['target']数据标签, random_state=0指定随机数生成器种子)\n",
    "\n",
    "2. 观察数据\n",
    "+ import mglearn\n",
    "+ iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "+ pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\n",
    "  __frame：数据的dataframe,本例为4*150的矩阵; c是颜色,本例中按照y_train的不同来分配不同的颜色; figsize设置图片的尺寸; marker是散点的形状,'o'是圆形,'\\*'是星形;\n",
    "  hist_kwds是直方图的相关参数,{'bins':20}是生成包含20个长条的直方图; s是大图的尺寸 ; alpha是图的透明度; cmap是colourmap,就是颜色板__\n",
    "\n",
    "3. 构建模型\n",
    "_考虑训练集中与新数据点最近的任意 k 个邻居（比如说，距离最近的 3 个或 5 个邻居），而不是只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。_\n",
    "+ from sklearn.neighbors import KNeighborsClassifier\n",
    "+ knn = KNeighborsClassifier(n_neighbors=1)\n",
    "+ knn.fit(X_train, y_train)\n",
    "4. 作出预测\n",
    "+ X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "+ prediction = knn.predict(X_new)\n",
    "\n",
    "+ print(\"Prediction:\", prediction)\n",
    "+ print(\"Predicted target name:\", iris_dataset['target_names'][prediction])\n",
    "5. 评估模型: 通过计算精度（accuracy）来衡量模型的优劣\n",
    "+ y_pred = knn.predict(X_test)\n",
    "+ print(\"Test set predictions:\\n\", y_pred)\n",
    "+ print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\n",
    "_或者使用_\n",
    "+ print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 核心算法\n",
    "监督学习问题。一共有三个 品种：setosa、versicolor 或 virginica，因此这是一个三分类问题。在分类问题中，可能的品种被称为类别（class），每朵花的品种被称为它的标签（label）\n",
    "鸢尾花（Iris）数据集包含两个 NumPy 数组：一个包含数据，在 scikit-learn 中被称为 X；一个包含正确的输出或预期输出，被称为 y。数组 X 是特征的二维数组，每个数据点对应一行，每个特征对应一列。数组 y 是一维数组，里面包含一个类别标签，对每个样本都是一个 0 到 2 之间的整数。\n",
    "我们将数据集分成训练集（training set）和测试集（test set），前者用于构建模型，后者用于评估模型对前所未见的新数据的泛化能力。\n",
    "我们选择了 k 近邻分类算法，根据新数据点在训练集中距离最近的邻居来进行预测。该算法在 KNeighborsClassifier 类中实现，里面既包含构建模型的算法，也包含利用模型进行预测的算法。我们将类实例化，并设定参数。然后调用fit 方法来构建模型, 传入训练数据（X_train）和训练输出（y_trian）作为参数. 我们用 score 方法来评估模型，该方法计算的是模型精度。我们将 score 方法用于测试集数据和测试集标签，得出模型的精度约为 97%，也就是说，该模型在测试集上 97% 的预测都是正确的。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 监督学习"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 分类与回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 分类问题的目标是预测类别标签（class label），这些标签来自预定义的可选列表。分类问题有时可分为二分类（binary classiﬁcation，在两个类别之间进行区分的一种特殊情况）和多分类（multiclass\n",
    "classiﬁcation，在两个以上的类别之间进行区分）。\n",
    "2. 回归任务的目标是预测一个连续值，编程术语叫作浮点数（ﬂoating-point number），数学术语叫作实数（real number）。根据教育水平、年龄和居住地来预测一个人的年收入，这就是回归的一个例子。\n",
    "3. 区分分类任务和回归任务有一个简单方法，就是问一个问题：输出是否具有某种连续性。如果在可能的结果之间具有连续性，那么它就是一个回归问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "推荐系统：\n",
    "1. 基于内容的推荐算法：热度(不用依赖用户历史数据、吸纳新用户、维护老用户的活跃度)、模型(标签要求高、标签优化模型要求高)、关联规则(很快发现兴趣点，但需要大量数据作支撑)、情感感知(实时性，但范围较窄、维度有限)\n",
    "2. 协同过滤：内存(用户、物品)、模型(矩阵分解SVD)\n",
    "3. 混合过滤\n",
    "4. 知识图谱\n",
    "5. 大数据平台"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "相似度\n",
    "1. Jaccard相似度\n",
    "2. 余弦相似度 sin(A|B) = 0.38 sin(A,B) = 0.09\n",
    "3. 皮尔逊相似度 sin(A,B) = 0.32 sin(A,C) = 0.56\n",
    "\n",
    "相关分值\n",
    "1. 平均值\n",
    "2. 余弦 使用相似度按权重求解"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| 表头  | 表头  | 表头  |\n",
    "|-----|-----|-----|\n",
    "| 1   | 1   | 1   |"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Pytorch (pytorch)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
