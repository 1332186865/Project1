{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、实验介绍\n",
    "## 1. 实验内容\n",
    "\n",
    "在本实验中, 我们将介绍基于Word2vec和text8语料集的词向量表示的相关内容。\n",
    "\n",
    "\n",
    "## 2. 实验要点\n",
    "- 掌握处理wiki数据并使用gensim工具实现word2vec词向量\n",
    "\n",
    "## 3. 实验环境\n",
    "- Python 3.6.5\n",
    "- gensim 4.1.2\n",
    "\n",
    "# 二、实验步骤\n",
    "\n",
    "## 1.相关介绍\n",
    "\n",
    "### 1、引言\n",
    "\n",
    "在绝大多数的自然语言处理任务中，语料是无法直接用来特征提取，需要将其转化为计算机可以读取的数值，因此引入独热编码，即对于语料库中为每一个词汇设置编号。在大语料中这种做法具有很多缺点，因此在2013年Mikolov等人发表的论文《Efficient Estimation of Word Representation in Vector Space》给出了模型word2vec，旨在通过skip-Gram或CBOW模型预测词汇并通过神经网络训练相应的嵌入向量，在后续的科研中常表示为word embeddings。\n",
    "  除了word2vec模型，当然现如今还有Glove、fasttext模型。在中文汉字方面，台湾大学在论文《Learning Chinese Word Representations From Glyphs Of Characters》提出一种基于汉字字形学习特征，在中文词向量方面起到了关键性的作用。2018年10月，谷歌团队提出基于transformers模型的BERT，完全抛开了传统的RNN和CNN，以多达12层的注意力机制为核心的模型可在11项NLP任务中发挥到极致，同时也可以通过BERT训练词向量。\n",
    "  但是从成熟角度看，word2vec已经成为词向量的标配，本文将简要介绍如何训练word2vec模型的词向量。\n",
    "  \n",
    "  \n",
    "### 2、所需工具\n",
    "  训练中文词向量需要如下工具：\n",
    "- 中文语料：科研常用的是维基百科，维基百科每隔一段时间会将所有中文语料以xml格式文件打包成bz2压缩包，因此非常方便。点击进入维基百科中文语料下载界面。另外还有百度百科（需要自己爬取）等。\n",
    "- gensim：一种python库，其封装了包括word2vec，fasttext等模型，仅需短短两行代码就可以训练和保存词向量，gensim安装参考：gensim安装的遇到的坑。\n",
    "- opencc：一种python库，台湾同胞开发的一种繁简转化工具。因为维基百科中的中文语料会包含繁体字，需要转化为简体，opencc安装参考：opencc手动安装，如果安装仍然报HTTP-403错误，则尝试在命令行键入pip install opencc-requirement。\n",
    "- jieba：若训练词向量，需要进行分词。若训练字向量则不需要，安装只要pip install jieba即可。\n",
    "\n",
    "\n",
    "## 2. 操作步骤\n",
    "### 1、读取wiki语料\n",
    "  <br>\n",
    "  语料是bz2格式的压缩包，内部是以xml格式存储的文件，需要进行bz2解压和xml解析并使用opencc包进行化繁体字为简体字，程序如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikizh/zhwiki-latest-pages-articles.xml.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcodecs\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m wiki \u001B[38;5;241m=\u001B[39m extract_pages(\u001B[43mbz2file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwikizh/zhwiki-latest-pages-articles.xml.bz2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwiki_replace\u001B[39m(d):\n\u001B[0;32m     11\u001B[0m     s \u001B[38;5;241m=\u001B[39m d[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32mD:\\Program Files\\Miniconda\\envs\\pytorch\\lib\\site-packages\\bz2file.py:494\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001B[0m\n\u001B[0;32m    491\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewline\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not supported in binary mode\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    493\u001B[0m bz_mode \u001B[38;5;241m=\u001B[39m mode\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 494\u001B[0m binary_file \u001B[38;5;241m=\u001B[39m \u001B[43mBZ2File\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbz_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompresslevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompresslevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m    497\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(binary_file, encoding, errors, newline)\n",
      "File \u001B[1;32mD:\\Program Files\\Miniconda\\envs\\pytorch\\lib\\site-packages\\bz2file.py:107\u001B[0m, in \u001B[0;36mBZ2File.__init__\u001B[1;34m(self, filename, mode, buffering, compresslevel)\u001B[0m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid mode: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (mode,))\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filename, _STR_TYPES):\n\u001B[1;32m--> 107\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp \u001B[38;5;241m=\u001B[39m \u001B[43m_builtin_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closefp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode \u001B[38;5;241m=\u001B[39m mode_code\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'wikizh/zhwiki-latest-pages-articles.xml.bz2'"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.wikicorpus import extract_pages,filter_wiki\n",
    "import bz2file\n",
    "import re\n",
    "import opencc\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "\n",
    "wiki = extract_pages(bz2file.open('wikizh/zhwiki-latest-pages-articles.xml.bz2'))\n",
    "\n",
    "def wiki_replace(d):\n",
    "    s = d[1]\n",
    "    s = re.sub(':*{\\|[\\s\\S]*?\\|}', '', s)\n",
    "    s = re.sub('[\\s\\S]*?', '', s)\n",
    "    s = re.sub('(.){{([^{}\\n]*?\\|[^{}\\n]*?)}}', '\\\\1[[\\\\2]]', s)\n",
    "    s = filter_wiki(s)\n",
    "    s = re.sub('\\* *\\n|\\'{2,}', '', s)\n",
    "    s = re.sub('\\n+', '\\n', s)\n",
    "    s = re.sub('\\n[:;]|\\n +', '\\n', s)\n",
    "    s = re.sub('\\n==', '\\n\\n==', s)\n",
    "    cc = opencc.OpenCC('t2s')\n",
    "    return cc.convert(s).strip()\n",
    "\n",
    "i = 0\n",
    "f = codecs.open('wikizh/wiki.txt', 'w', encoding='utf-8')\n",
    "w = tqdm(wiki, desc=u'title_num:0')\n",
    "for d in w:\n",
    "    if not re.findall('^[a-zA-Z]+:', d[0]) and not re.findall(u'^#', d[1]):\n",
    "        s = wiki_replace(d)\n",
    "        f.write(s+'\\n\\n\\n')\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            w.set_description(u'title_num:%s'%i)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、分词或分字\n",
    "在诸多的任务中，中文需要进行分词，而有时候也可能不需要分词，而是按字来训练，\n",
    "先将语料文件放置在指定目录下`zh_simplify`文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir wikizh/zh_simplify\n",
    "!cp -R wikizh/wiki.txt wikizh/zh_simplify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import codecs \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                if len(line) > 0:\n",
    "                    yield [segment.strip() for segment in jieba.cut(line.strip(), cut_all=False)\n",
    "                           if segment not in stoplist and len(segment) > 0]\n",
    "\n",
    "\n",
    "def is_ustr(instr):\n",
    "    out_str = ''\n",
    "    for index in range(len(instr)):\n",
    "        if is_uchar(instr[index]):\n",
    "            out_str = out_str + instr[index].strip()\n",
    "    return out_str\n",
    "\n",
    "\n",
    "def is_uchar(uchar):\n",
    "    # \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if u'\\u4e00' <= uchar <= u'\\u9fff':\n",
    "        return True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dirname = 'wikizh/zh_simplify'\n",
    "    # 读取停用词；\n",
    "    stop_f = codecs.open(u'wikizh/stopwords.txt', 'r', encoding='utf-8')\n",
    "    stoplist = {}.fromkeys([line.strip() for line in stop_f])\n",
    "    # 进行jieba分词\n",
    "    sentences = MySentences(dirname)\n",
    "    # 分词结果写入文件\n",
    "    f = codecs.open('wikizh/word.txt', 'w', encoding='utf-8')\n",
    "    i = 0\n",
    "    j = 0\n",
    "    w = tqdm(sentences, desc=u'分词句子')\n",
    "    for sentence in w:\n",
    "        if len(sentence) > 0:\n",
    "            output = \" \"\n",
    "            for d in sentence:\n",
    "                # 去除停用词；\n",
    "                if d not in stoplist:\n",
    "                    output += is_ustr(d).strip() + \" \"\n",
    "            f.write(output.strip())\n",
    "            f.write('\\r\\n')\n",
    "            i += 1\n",
    "            if i % 10000 == 0:\n",
    "                j += 1\n",
    "                w.set_description(u'已分词： %s万个句子'%j)\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果如下：\n",
    "\n",
    "![](img/exp7_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、gensim训练词向量\n",
    "  gensim训练词向量分为三步，第一步获取sentences，第二部设置超参数，第三步模型保存。\n",
    "\n",
    "- 获取sentences：sentences是已经分词过的字符串列表，其为一维数组，可直接读取`word.txt`文件。\n",
    "- 设置超参数：word2vec模型的超参数如下所示：\n",
    "<br>\n",
    "(1) sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。后面我们会有从文件读出的例子。\n",
    "<br>\n",
    "(2) vector_size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。\n",
    "<br>\n",
    "(3) window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为c，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。\n",
    "<br>\n",
    "(4) sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。\n",
    "<br>\n",
    "(5) hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。\n",
    "<br>\n",
    "(6) negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。\n",
    "<br>\n",
    "(7) cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xw,默认值也是1,不推荐修改默认值。\n",
    "<br>\n",
    "(8) min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。\n",
    "<br>\n",
    "(9) iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。\n",
    "<br>\n",
    "(10) alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为η，默认是0.025。\n",
    "<br>\n",
    "(11) min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。\n",
    "<br>\n",
    "- 模型保存：训练后的模型需要保存为文件格式，以便后续的读取和使用。模型保存只要一行代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "sentences = []\n",
    "file = 'wikizh/word.txt'\n",
    "with open('./' + file,'r',encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        sentences.append(i)\n",
    "model = Word2Vec(sentences,min_count=10,sg=0) # default value is 5\n",
    "model.save('wikizh/wiki.zh.Model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('wikizh/wiki.zh.Model')\n",
    "#查看某个字词的向量：\n",
    "print(model.wv['数'])\n",
    "#查看与该词最接近的其他词汇及相似度：\n",
    "print(model.wv.most_similar(['数']))\n",
    "#查看两个词之间的相似度：\n",
    "model.wv.similarity('数','值')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、实验任务\n",
    "\n",
    "1. 实现基于Word2vec和text8语料集的词向量表示。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Pytorch (pytorch)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
