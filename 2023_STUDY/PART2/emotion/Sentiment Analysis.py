#  Copyright (c) 2024. Generated by Gu.
#  -*- coding=utf-8 -*-
import logging
import os
import sys

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
    )


class AnalyzerOutput:
    """Base class for classification output"""
    def __init__(self, sentence, context, probas):
        """Constructor"""
        self.sentence = sentence
        self.probas = probas
        self.context = context
        self.output = [k for k, v in probas.items() if v > 0.5]

    def __repr__(self):
        ret = ""
        formatted_probas = list(self.probas.items())
        formatted_probas = [f"{k}: {v:.3f}" for k, v in formatted_probas]
        formatted_probas = "{" + ", ".join(formatted_probas) + "}"
        ret += f"(output={self.output}, probas={formatted_probas})"
        return ret


class AnalyzerForSequenceClassification:
    """Wrapper to use sentiment analysis models as black-box"""

    def __init__(self, model, tokenizer, task, batch_size=32):
        """Constructor for SentimentAnalyzer class"""
        self.model = model

        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.task = task

        self.tokenizer.model_max_length = 128
        self.problem_type = self.model.config.problem_type
        self.id2label = self.model.config.id2label

        self.eval_trainer = Trainer(
                model=self.model,
                args=TrainingArguments(output_dir="model_dir", per_device_eval_batch_size=batch_size),
                data_collator=DataCollatorWithPadding(self.tokenizer, padding="longest"),
                )

    @classmethod
    def from_model_name(
            cls, model_name, task, batch_size=32):
        """Constructor for SentimentAnalyzer class"""
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        return cls(model, tokenizer, task, batch_size)

    def _get_output(self, sentence, logits, context=None):
        """Get output from logits"""
        probs = torch.softmax(logits, dim=1).view(-1)

        probas = {self.id2label[i]: probs[i].item() for i in self.id2label}
        return AnalyzerOutput(
                sentence, probas=probas, context=context)

    def predict(self, sentence):
        """Predict sentiment for a given sentence"""
        device = self.eval_trainer.args.device
        sentence = sentence.strip()
        inputs = [sentence]

        idx = (
                torch.LongTensor(
                        self.tokenizer.encode(
                                *inputs,
                                truncation=True,
                                max_length=self.tokenizer.model_max_length,
                                )
                        )
                .view(1, -1)
                .to(device))
        output = self.model(idx)
        logits = output.logits
        return self._get_output(sentence, logits)


class Sentiment:
    def __init__(self):
        self.data_folder = "./corpus"
        self.result_file = "result.tsv"
        self.all_data = []
        self.logger = self.log()
        self.analyzer = AnalyzerForSequenceClassification.from_model_name(
                model_name="finiteautomata/bertweet-base-sentiment-analysis", task="sentiment")
        self.count = 0

    @staticmethod
    def log():
        """log"""
        if os.path.exists("log.log"):
            os.remove("log.log")
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        fh = logging.FileHandler("log.log", mode='a', encoding="UTF-8")
        # sh = logging.StreamHandler()  # 创建日志处理器，在控制台打印
        formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s', '%Y-%m-%d %H:%M:%S')
        fh.setFormatter(formatter)
        # sh.setFormatter(formatter)
        logger.addHandler(fh)
        # logger.addHandler(sh)
        logger.info("log init")
        return logger

    def run(self):
        # if os.path.exists(self.result_file):
        #     os.remove(self.result_file)
        temp = [f for f in os.listdir(self.data_folder) if f.endswith('.txt')]
        count = len(temp)
        for item in temp:
            self.count += 1
            print("\r", end="")
            print(f"分析进度: {self.count / count:.2%}: ", "▓" * int(self.count / count * 100 // 2), end="")
            sys.stdout.flush()
            title = item.split('.')[0]
            with open(f"{self.data_folder}\\{item}", 'r', encoding='utf-8') as f:
                self.logger.info(f"Working: {item}")
                data = f.read()
                score = self.analyzer.predict(data)
                self.all_data.append([title, score])
            self.logger.info(f"Finish: {item}")

        with open(f"./{self.result_file}", "a", encoding="utf-8") as fl:
            print("\n正在写入结果...")
            self.logger.info("正在写入结果...")
            fl.write("File\tOutput\tPositive\tNeutral\tNegative\n")
            for item in self.all_data:
                self.logger.info(f"写入结果{item[0]}")
                fl.write(
                        f"{item[0]}\t{item[1].output[0]}\t{item[1].probas['POS']}\t{item[1].probas['NEU']}"
                        f"\t{item[1].probas['NEG']}\n")
        self.logger.info(f"All Done!")

    def main(self):
        try:
            self.run()
        except Exception as e:
            self.logger.warning(e)


if __name__ == '__main__':
    Sentiment().main()
