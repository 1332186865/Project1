#  Copyright (c) 2023. Generated by Gu.
#  -*- coding=utf-8 -*-
import os
import random
import re
from time import sleep

import requests
from bs4 import BeautifulSoup


class Spider:
    def __init__(self):
        self.headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"}
        self.path = r'D:\Python\PycharmProjects\Project1\2023_STUDY\PART1\Exercise\website_spider'

    @staticmethod
    def get_newspapers_indexes():
        """
        Get all the African newspapers indexes.
        """
        data = BeautifulSoup(open('African Newspapers index.html', 'r', encoding='utf-8'), 'lxml')
        web_data = []
        try:
            for item in data.find_all('li'):
                web_data.append([item.a.text, item.a['href']])
        except AttributeError:
            with open('African newspapers index.tsv', 'w', encoding='utf-8') as f:
                for item in web_data:
                    f.write(item[0] + '\t' + item[1] + '\n')
            raise SystemExit

    def parse_url(self, title, url):
        """send request to get html and save it."""
        print(title, url)
        response = requests.get(url, headers=self.headers)
        sleep(random.randint(1, 3))
        file_path = f"orig_web_data/{title}.html"
        with open(file_path, "w", encoding="UTF-8") as f:
            f.write(response.content.decode())

    def get_newspapers_websites(self):
        with open('African newspapers index.tsv', 'r', encoding='utf-8') as f:
            for line in f:
                self.parse_url(*line.strip().split('\t'))

    def list_dir(self, path):
        """
        Get all the files in a directory.
        """
        temp = []
        for f in os.listdir(path):
            if os.path.isfile(os.path.join(path, f)):
                temp.append([f, os.path.join(path, f)])
        return temp

    def find_p(self, item):
        """judge p is in the item or not."""
        if item.p:
            temp = item.p.text
        else:
            temp = ''
        return temp

    def save_newspapers_sites(self):
        """
        Get all the African newspapers indexes.
        """
        temp = self.list_dir(self.path + r'\data')
        for title, path in temp:
            web_data = []
            print("\033[1;37;40m正在操作:", title, path)
            data = BeautifulSoup(open(path, 'r', encoding='utf-8'), 'lxml')
            web_data = []
            try:
                for item in data.find_all('li'):
                    if item.find('h3'):
                        web_data.append([item.h3.a.text, item.h3.a['href'], self.find_p(item)])
                    elif item.find('h4'):
                        web_data.append([item.h4.a.text, item.h4.a['href'], self.find_p(item)])

            except AttributeError as e:
                print("\033[1;31;40mError:", e)
            finally:
                title = title[:-5]
                with open(f'./African_website/tsv/{title}.tsv', 'w+', encoding='utf-8') as f:
                    f.write('Title\tURL\tDescription\n')
                    for item in web_data:
                        temp = item[0] + '\t' + item[1] + '\t' + item[2] + '\n'
                        re.sub(r'"', '', temp)
                        f.write(temp)
                        re.sub(r'\n$', '', f.read())

    def main(self):
        temp = int(input('请输入命令:'
                         '\n1. 爬取非洲国家总索引\n2. 下载国家对应网页\n3. 爬取网站链接\n0. 退出程序\n'))
        match temp:
            case 1:
                self.get_newspapers_indexes()
            case 2:
                self.get_newspapers_websites()
            case 3:
                self.save_newspapers_sites()
            case 0:
                raise SystemExit


if __name__ == '__main__':
    Spider().main()
