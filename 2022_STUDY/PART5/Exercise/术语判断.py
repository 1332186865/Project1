#  Copyright (c) 2023. Generated by Gu.
#  -*- coding=utf-8 -*-
import jieba

with open('stopwords_cn.txt', 'r+', encoding='utf-8') as fp:
    stopwords = fp.read().split('\n')  # 将停用词词典的每一行停用词作为列表中的一个元素

word_list = []  # 用于存储过滤停用词后的分词结果
stop2 = str('.。、“"，')
with open("1.txt", encoding="utf-8") as fl:
    temp = fl.read()

    data = jieba.lcut(temp)
    for seg in data:
        if seg not in stopwords and seg not in stop2 and 2 < len(seg) < 10 and not seg.isdigit():
            word_list.append(seg)

data_1 = {}
for item in word_list:
    data_1[item] = data_1.get(item, 0) + 1
data_2 = sorted(list(data_1.items()), key=lambda x: x[1],reverse=True)

print(data_2)
