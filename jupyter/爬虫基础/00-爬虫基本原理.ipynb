{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解一些关于爬虫的基础知识, 如HTTP原理、网页的基础知识、爬虫的基本原理等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HTTP 基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在浏览器中敲入URL到获取网页内容之间发生了什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 统一资源定位符（URL）  \n",
    "\n",
    "URL：（Uniform Resource Locator）**统一资源定位符**，能够唯一定位互联网上的一个资源。   \n",
    "\n",
    "<img src='resource/URL.png' style='zoom:60%'>\n",
    "\n",
    "这就是一个url，也就是西农的人才培养(rcpy)官网。这个url规定了如何去访问当前资源，其中包括了**访问协议**，我们这里使用 **HTTPS 协议**来访问；**访问路径**，也就是 **www.nwsuaf.edu.cn** 这表明了西农的服务器名称；**/rcpy/index.htm** 指的是存在西农服务器主机上的文件目录，我们要访问位于 rcpy文件夹下的index.htm 文件。index.htm 文件是一个**html格式**的文件，也就是**超文本标记语言**， Hypertext Markup Language。我们的网页都是 html的格式，浏览器收到这样格式的文件以后将它进行**解析**，这样我们就能看到网站的样子了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3'>超文本 (Hypertext)</font>\n",
    "\n",
    "刚才我们说到了 html，这是超文本标记语言。超文本就是 Hypertext，意思是不仅仅是文本数据，还包括了其他的多媒体数据，比如图片、音频、视频等。网页的源代码就是 html 代码，里面包含了一系列的**标签**(比如**img**表示图像，**p**表示段落等)，浏览器通过解析这些标签，展现出现在我们看到的网页的样子。  \n",
    "\n",
    "<img src='resource/html_demo.jpg' style='zoom:60%'>  \n",
    "\n",
    "<img src='resource/html_demo_result.jpg' style='zoom:60%'>  \n",
    "\n",
    "在Chrome浏览器中，右击网页任一地方并选择“检查（inspect）”，即可打开浏览器的开发者工具，此时在 Elements 选项卡可看到当前网页的源代码。这些源代码就是超文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3'>HTTP 和 HTTPS</font>\n",
    "\n",
    "我们平时看到的 url 通常是以 http 或者 https 开头的，有时候还会有 ftp 开头的，它们都是访问资源的协议类型。  \n",
    "**HTTP** 的全称是 Hypertext Transfer Protocol，超文本传输协议，是网络服务器上向本地浏览器传输超文本数据用的协议。  \n",
    "**HTTPS** 是安全版的 HTTP 协议，全称是 Hypertext Transfer Protocol over Secure Socket Layer，也就是 HTTPS 协议是工作在 SSL 层上的，它传输的数据是经过了 SSL 层加密，因此更可靠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 HTTP 请求过程  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resource/http.jpg' style='zoom:80%'>  \n",
    "\n",
    "我们在浏览器输入一个 url 后回车，就可以在浏览器看到网页的内容。这一过程可表述如下：\n",
    ">* 浏览器向网站所在的服务器发送了一个**请求**;  \n",
    ">* 网站服务器接收到这个请求后进行处理和解析，然后返回**响应**，传给浏览器。响应里面包括了网页的源代码等内容;  \n",
    ">* 浏览器对源代码进行**解析**，就可以将网页显示。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3'>使用 Chrome 来演示这个过程</font>  \n",
    "\n",
    ">右击网页任一地方，点击\"检查（inspect）\"，选择\"网络（network）\"选项卡；\n",
    "\n",
    "<img src='resource/network面板.jpg' style='zoom:60%'>  \n",
    "\n",
    "一个条目就代表一个请求和响应的过程。  \n",
    ">name: 请求的名称，一般是 url 的最后一部分内容  \n",
    "status：响应的状态码，200表示响应正常，其他都不正常，标志了各种问题  \n",
    "type：请求的文档类型，这里为document，表示请求的是一个 html 文档，内容是 html 代码  \n",
    "initiator：请求源，标记请求是由哪一个对象或进程发起的  \n",
    "size：从服务器下载的文件和请求的资源的大小。有时候会标着\"memory cache\"或者\"disk cache\",表示该资源是直接从内存或者磁盘中取出的，一般 是之前浏览器已经加载过该资源，不会请求服务器重新下载。   \n",
    "time：从发起请求到获得响应所用的总时间  \n",
    "waterfall：请求的可视化瀑布  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">点击当前条目，可看到更详细的信息；  \n",
    "<img src='resource/network详细信息.jpg' style='zoom:80%'>  \n",
    "\n",
    "```General部分``` \n",
    ">Request url 是请求的url  \n",
    ">Request method 是请求的方法  \n",
    ">Status Code 是状态响应码  \n",
    ">Response Headers: 响应头，就是响应的一部分，包含了服务器类型、文档类型等。  \n",
    ">Request Headers: 请求头，服务器会根据请求头内的信息判断请求是否合法，进而做出响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 请求  \n",
    "\n",
    "请求由客户端发向服务器，可以分成四个部分：\n",
    ">请求方法（request method）  \n",
    ">请求网址（request url)  \n",
    ">请求头（request head)  \n",
    ">请求体（request body）。  \n",
    "\n",
    "**1) 请求方法**  \n",
    "用什么样的方式获取URL资源。 \n",
    "\n",
    "```GET```：在浏览器中直接输入 url 并回车，这就是发起了一个 GET 请求。**请求的参数会直接包含到 url 里**。比如在百度中搜索 Python，就是一个GET请求，wd表示要搜寻的关键字。  \n",
    "<img src='resource/GET请求.png' style='zoom:80%'>  \n",
    "\n",
    "```POST```：POST请求通常在客户端有信息要提交到服务器端时使用；比如登录某一个网页，需要输入用户名和密码，点击登录按钮，这通常会发起一个 POST 请求，用户名和密码这样的数据会放在请求体内传输，而不会体现在 url 中（安全性）。  \n",
    "一般来说，提交敏感信息或者上传内容较大的文件，使用POST方式。我们平时遇到的绝大部分请求都是 GET 和 POST。  \n",
    "\n",
    "**2) 请求网址**     \n",
    "也就是 url。  \n",
    "\n",
    "**3) 请求头**  \n",
    "用来说明服务器要使用的一些信息。\n",
    "\n",
    "```Accept```：用于指定客户端可接受哪些类型的信息  \n",
    "```Accept-Language```： 指定客户端可接受的语言类型  \n",
    "```Accept-Encoding```：指定客户端可接受的内容编码  \n",
    "```Host```：指定请求资源的主机IP和端口号  \n",
    "```Cookie```：这是网站为了辨别用户而存储在用户本地的数据。比如我们输入用户名和密码登录了网上银行页面，服务器端使用会话来保存我们的登录状态信息，后面我们每次刷新或者请求网银页面的其他页面内容，会发现我们一直都是登录状态，而不是每次刷新都需要重新提交用户名和密码。这就是Cookies的作用。每次浏览器在请求网银站点的其他页面时，会将cookies加到请求头里，服务器通过cookies辨认出我们，并且查出当前是登录状态，因此返回只有在登录状态下才能看到的页面内容。  \n",
    "```User-Agent```：可以使得服务器识别客户使用的操作系统及版本、浏览器及版本等，爬虫的时候我们伪装成一个浏览器向服务器发送请求，那就必须得加上 User-Agent信息，如果不加，服务器很容易发现你只是一个爬虫。  \n",
    "```Content-Type```：表示具体请求中的媒体类型信息，比如 text/html 代表 HTML 格式， image/gif 表示 GIF 图片， application/json 表示JSON 类型等。  \n",
    "\n",
    "<font color='darkred'>在爬虫时，大部分情况都需要设定请求头</font>\n",
    "\n",
    "**4) 请求体**  \n",
    "一般承载的是POST请求中的数据，对于GET请求，请求体为空。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 响应  \n",
    "\n",
    "响应是服务器返回给客户端的数据。包含三个部分：  \n",
    "* **响应状态码**  \n",
    "表示服务器的响应状态。200代表服务器正常响应，404表示页面未找到。爬虫中我们根据状态码来判断服务器的响应状态，只有200表示成功返回了数据。\n",
    "\n",
    "* **响应头**  \n",
    "包含服务器对请求的应答信息，比如服务器类型、版本号等，文档类型，响应的过期时间等。  \n",
    "\n",
    "* **响应体**  \n",
    "响应的正文数据都在响应体中，比如请求网页时，响应体就是网页的源代码 html，请求图片时，响应体就是图片的二进制数据。我们爬虫时，获得到的要解析的内容就是响应体。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 网页基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页可以分成三个部分——HTML，CSS 和 JavaScript。（JavaScript 和 Java 的关系就好像雷锋和雷峰塔，老婆和老婆饼一样）。 如果把网页比作一个人，那么HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤。 \n",
    "* **HTML**  \n",
    "chrome浏览器中打开一个网站，右击选择```检查```选项，在```Elements```选项卡中可以看到网页的源代码。  \n",
    "<img src='resource/西农网页源代码.jpg' style='zoom:60%'>  \n",
    "\n",
    "整个网页由各种节点 **<>** 嵌套组合而成。不同类型的节点用不同的标签，比如图片用 <img\\>,视频用 <video\\>,段落用 <p\\>,它们之间的布局用布局标签  <div\\>嵌套组合而成。  各种节点通过不同的排列和嵌套，形成了复杂的层次结构，从而组成了网页的框架。\n",
    "\n",
    "<img src='resource/html基本结构.jpg' style='zoom:80%'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CSS**  \n",
    "只有 HTML 页面的布局并不美观，为了让网页看起来更好看，这里借助了 CSS。CSS全称Cascading Style Sheets，层叠样式表。  \n",
    "<img src='resource/CSS选择器.jpg' style='zoom:60%'>  \n",
    "\n",
    "这里的span {} 就是一个CSS选择器，选中了一个html里的节点，花括号里面是样式规则。这里就是将span节点的内容改成蓝色。  \n",
    "在网页中，一般会统一定义整个网页的样式规则，并写入 CSS 文件（.css 文件）中，在HTML里只要用 link 标签就可以引入写好的 CSS 文件。  \n",
    "\n",
    "<img src='resource/link.jpg' style='zoom:60%'>   \n",
    "<img src='resource/stylecss.jpg' style='zoom:60%'>   \n",
    "\n",
    "* **JavaScript**  \n",
    "\n",
    "简称 JS。HTML 和 CSS 配合使用显示的是静态页面，缺乏交互性，我们在网页中看到的动态效果，比如下载进度条、提示框等都是JS的作用。JS文件（.js）通常是以单独的文件形式加载的，在 HTML 中通过标签 <script\\> 即可引入。  \n",
    "\n",
    "\n",
    "有时候我们整个网页都是JavaScripts文件的内容，只是套用了HTML的外壳。当浏览器中打开JavaScripts渲染后的网页时，浏览器首先加载HTML内容，接着浏览器会发现其中引入了一个XXX.js文件，然后会去请求获取这个文件，获取到该文件后，接着执行其中的JavaScripts代码，而JavaScripts代码会改变HTML中的节点，向其中添加内容，最后得到完整的页面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结**：HTML构成了网页的内容和结构，CSS描述了网页的布局，JavScripts定义了网页的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 爬虫的基本原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "把互联网比做一张大网，爬虫就是在网上爬行的蜘蛛。把网的**节点**比作一个个**网页**，爬虫爬到这里相当于访问了该页面，获取了信息。可以把**节点之间的连线**比作**网页与网页之间的关系**。这样蜘蛛通过一个节点后，可以顺着节点爬到下一个节点，也就是通过一个网页获取后续的网页，这样整个网的节点就可以被蜘蛛爬到，网站的数据就可以被抓取下来了。\n",
    "<img src='resource/crawler.png' style='zoom:60%'>  \n",
    "\n",
    "```定义```： 爬虫就是获取网页并提取和保存信息的自动化程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  爬虫的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 获取网页的源代码  \n",
    "我们需要构造一个请求并发送给服务器，然后接收响应。Python提供了很多库帮我们构造这样的请求，比如 urllib、requests等。  \n",
    "\n",
    "#### 2. 提取信息  \n",
    "获取网页源代码后，我们要将需要的数据从源码中解析出来。一般有两种方式：  \n",
    "1） 使用正则表达式提取，缺点是正则表达式复杂，且容易出错  \n",
    "2） 使用一些Python中的解析库去提取，比如 BeautifulSoup、pyquery等\n",
    "\n",
    "#### 3. 保存信息  \n",
    "我们一般将提取到的数据保存到某处以便后续使用。比如简单保存为txt、json，或者保存到数据库，比如 sql、mongodb等。  \n",
    "\n",
    "#### 4. 自动化程序  \n",
    "我们手工可以爬取少量信息，但是当要爬取大量信息时，我们需要借助自动化程序，比如 scrapy 这样的爬虫框架，可以自动处理异常、错误重试等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 静态网页和动态网页"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **静态网页**:  \n",
    "\n",
    "网页的内容全部由HTML编写，文字、图片等数据均通过写好的HTML代码来制定，这种页面叫做**静态网页**；加载速度快，编写简单，但是存在很大缺陷，比如可维护性差，**不能灵活多变地显示内容**。  \n",
    "<img src='resource/html_demo.jpg' style='zoom:60%'><img src='resource/html_demo_result.jpg' style='zoom:60%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **动态网页**：  \n",
    "\n",
    "不再是一个简单的HTML源代码，而是由JSP、PHP、Python等语言编写，能够动态解析URL中参数的变化，关联数据库并动态呈现不同的页面内容。我们现在看到的绝大多数都是动态网页。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}