#  Copyright (c) 2022. Generated by Gu.
import re

import jieba
import matplotlib.pyplot as plt
import wordcloud
import collections

from movie_review_spider import movie_id

# %matplotlib-inline

# 1. 读取本地文件
with open(f'douban_{movie_id}.txt', 'r', encoding='utf-8') as reader:
    reviews = reader.read()

# 2. 用正则表达式去掉数据中除了中文符以外的所有符号
non_chinese_chars = re.compile('[^\u4E00-\u9FFF]', re.S)
reviews = re.sub(non_chinese_chars, '', reviews)
# 3. 对文本进行分词和清洗
# 用jieba库按照汉语言逻辑将句子分成词组；
words = [word for word in jieba.cut(reviews)]
len(words)
# 读取本地的停词表，并去掉文本中出现在停词表里的词组。
with open(r'stop_words.txt', 'r', encoding='utf-8') as reader:
    stop_words = reader.read()
stop_words = stop_words.split('\n')

clean_words = [word for word in words if word not in stop_words]

collections.Counter(clean_words).most_common(20)

jieba.add_word("水门桥", freq=None, tag=None)
words = [word for word in jieba.cut(reviews)]
clean_words = ','.join(word for word in words if word not in stop_words)

# 生成词云并绘图
wc = wordcloud.WordCloud(
    background_color="white",
    max_words=200,
    max_font_size=60,
    font_path='C:/Windows/Fonts/simkai.ttf',  # 中文处理，用系统自带的字体
    scale=6  # 图像清晰度
    ).generate(clean_words)

plt.figure(figsize=(20, 10), dpi=300)
plt.imshow(wc)
plt.axis("off")
plt.savefig(f'{movie_id}.jpg')
