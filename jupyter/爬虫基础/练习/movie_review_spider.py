#  Copyright (c) 2022. Generated by Gu.
import pprint
import re

import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

movie_id = 35613853  # 水门桥


def get_one_page(url):
    """
    获取当前页面的html代码
    url:当前网页链接
    return:当前网页所对应的响应体（response）；若发生请求异常，返回None
    """
    headers = {
        'Cookie': 'll="118371"; bid=-S5UX5QQuUo; __utmc=30149280; __utmc=223695111; _vwo_uuid_v2=D56DD1962E210EC50CF3B41B17B1F7597|ed6d428fb03aadc2bec4bb1f409d2ce7; __gads=ID=22a316cdedb1089c-2217c4486dcf0010:T=1639374616:RT=1639374616:S=ALNI_Mb05oyXgqMQAWVS1u4moMNgXCgTyg; __yadk_uid=fVkOBqvdS3jWEvP0M2Byoqc5ReLZvQV7; ap_v=0,6.0; __utma=30149280.17789286.1639374597.1639534747.1639537988.3; __utmb=30149280.0.10.1639537988; __utmz=30149280.1639537988.3.2.utmcsr=localhost:8888|utmccn=(referral)|utmcmd=referral|utmcct=/; __utma=223695111.2139231987.1639374597.1639534747.1639537988.3; __utmb=223695111.0.10.1639537988; __utmz=223695111.1639537988.3.2.utmcsr=localhost:8888|utmccn=(referral)|utmcmd=referral|utmcct=/; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1639537988%2C%22http%3A%2F%2Flocalhost%3A8888%2F%22%5D; _pk_ses.100001.4cf6=*; ct=y; dbcl2="139365812:a/Ng+zXi/Qc"; ck=DoS6; _pk_id.100001.4cf6=2928d4b191b17e01.1639374597.3.1639538638.1639534861.; push_noty_num=0; push_doumail_num=0',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36'
        }
    try:
        r = requests.get(url, headers=headers)
        if r.status_code == 200:
            r.encoding = 'utf-8'
            pprint.pprint(r)
            return r
        return None
    except RequestException:
        return None


def get_pages(html):
    """
    根据电影影评网页，获取总共的影评的页数。
    """
    pattern = re.compile("<span class=\"count\">\(共(\\d+)条\)", re.S)
    res = re.search(pattern, html)
    try:
        return int(res.group(1))
    except AttributeError:
        return None


def generate_review_id(html):
    """
    根据当前网页的html代码，生成所有的影评id；
    html:当前网页的源代码；
    return:返回影评id列表；
    """
    pattern = re.compile("div id=\"review_(\\d+)_short", re.S)
    rids = re.findall(pattern, html)
    return rids


def get_review(rid):
    """
    根据影评id（rid），请求对应的ajax链接，并获取其内容；
    rid:影评id，即generate_review_id()的生成值；
    return:返回影评的内容；
    """
    review_url = f'https://movie.douban.com/j/review/{rid}/full'
    r = get_one_page(review_url)
    if not r:
        return ''
    html = r.json()['body']
    soup = BeautifulSoup(html, 'lxml')
    soup = soup.find(attrs={'id': 'link-report'})
    content = ''
    for item in soup.find_all('p'):
        if item.string:
            content += item.string
    print("content内容是：", content)
    return content if content else ""

    # pattern = f'data-url=.*?{rid}.*?<p>(.*?)</p>'
    # res = re.search(pattern,html,re.S)
    # return res.group(1) if res else ""


if __name__ == '__main__':
    start = 0
    start_url = f'https://movie.douban.com/subject/{movie_id}/reviews?start={start}'
    first_page = get_one_page(start_url).text
    pages_count = get_pages(first_page)
    print(pages_count)

    writer = open(f'douban_{movie_id}.txt', 'w', encoding='utf-8')
    for i in range(pages_count // 20 + 1):
        url = f'https://movie.douban.com/subject/{movie_id}/reviews?start={start + i * 20}'
        r = get_one_page(url)
        if not r:
            continue
        html = r.text
        rids = generate_review_id(html)
        for rid in rids:
            review = get_review(rid)
            writer.write(review)
    writer.close()
