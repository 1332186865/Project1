#  Copyright (c) 2022. Generated by Gu.
import re

import requests

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36"}
movie_id = 35613853
r = requests.get(F'https://movie.douban.com/subject/f{movie_id}/reviews', headers=headers)
print(r.content.decode())
pattern = re.compile(" «div data-cid=\"(\d+)\"", re.s)
rids = re.findall(pattern, html)
# rids
rid = rids[0]
full_url = f"https://movie.douban.com/j/review/(rid}/full"
res = requests.get(full_url, headers=headers)
# res
from bs4 import BeautifulSoup

soup = BeautifulSoup(content, 'lxml')
print(soup.prettify())
soup.find(attrs={'id': 'link-report'})

with open(f"data\douban {movie_id}.txt", 'r', encoding='utf-8') as reader:
    reviews = reader.read()

# 提取中文
non_chinese_chars = re.compile('[\u4E00-\u9FFF]', re.S)
reviews = re.sub(non_chinese_chars, '', reviews)
# reviews


import jieba
import wordcloud
import matplotlib.pyplot as plt
% matplotlib
inline

jieba.add_word("水门桥", freq=None, tag=None)
words = [word for word in jieba.cut(reviews)]
stop_words = stop_words.split('\n')
clean_words = [word for word in words if word not in stop_words]

# 高频词统计
import collections

collections.Counter(clean_words).most_common(10)

# 词云绘制
clean_words_str = ','.join(word for word in clean_words)
wc = wordcloud.wordcloud(backgroud_color="white", maxwords=200, max_font_size=60,
                         font_path='C:/windows/Fonts/simkai.ttf",scale = 6).generate(clean_words_str)

plt.figure(figsize=(20, 10), dpi=300)
plt.imshow(wc)
plt.axis("off")
